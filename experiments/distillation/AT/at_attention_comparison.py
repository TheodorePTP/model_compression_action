```
è„šæœ¬ç”¨é€”ï¼š
https://blog.csdn.net/qq_44923064/article/details/155104865?fromshare=blogdetail&sharetype=blogdetail&sharerId=155104865&sharerefer=PC&sharesource=qq_44923064&sharefrom=from_link
ä¸Šæ–‡å›¾3å’Œå›¾4å›¾5çš„ç”Ÿæˆè„šæœ¬

ä½¿ç”¨æ–¹æ³•ï¼š
python at_attention_comparison.py

```

import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
from torchvision import models

# ----------------------------
# 1. æ¨¡æ‹Ÿæ•™å¸ˆå’Œå­¦ç”Ÿç‰¹å¾å›¾ï¼ˆç®€åŒ–ç‰ˆï¼‰
# ----------------------------
def get_dummy_features(batch=1, c_t=512, c_s=256, h=14, w=14):
    """ç”Ÿæˆæ¨¡æ‹Ÿçš„æ•™å¸ˆå’Œå­¦ç”Ÿç‰¹å¾å›¾"""
    torch.manual_seed(42)
    f_t = torch.randn(batch, c_t, h, w) * 0.5 + 1.0  # æ•™å¸ˆç‰¹å¾ï¼ˆæ›´â€œèšç„¦â€ï¼‰
    f_s = torch.randn(batch, c_s, h, w) * 1.0        # å­¦ç”Ÿç‰¹å¾ï¼ˆæ›´â€œåˆ†æ•£â€ï¼‰
    return f_s, f_t

def compute_attention_map(feat, p=2):
    """è®¡ç®— AT è®ºæ–‡ä¸­çš„æ³¨æ„åŠ›å›¾: sum of power-p across channels"""
    att = feat.pow(p).mean(dim=1)  # [B, H, W]
    # Normalize each attention map independently, without flattening it
    att_normalized = F.normalize(att.view(att.size(0), -1), p=2, dim=1).view_as(att)
    return att_normalized  # keep [B, H, W]


# ----------------------------
# å›¾1ï¼šæ•™å¸ˆ vs å­¦ç”Ÿæ³¨æ„åŠ›å›¾
# ----------------------------
def plot_attention_comparison():
    # å‡è®¾ get_dummy_features æ˜¯ä¸€ä¸ªå·²å®šä¹‰çš„å‡½æ•°ï¼Œå®ƒè¿”å›å­¦ç”Ÿå’Œæ•™å¸ˆæ¨¡å‹çš„ç‰¹å¾
    # å¦‚æœæœ‰çœŸå®çš„ç‰¹å¾ï¼Œè¯·ç›´æ¥ä½¿ç”¨å®ƒä»¬ä»£æ›¿è¿™ä¸ªå‡è®¾å‡½æ•°
    def get_dummy_features(h=28, w=28):
        # ç¤ºä¾‹æ•°æ®ï¼Œå®é™…ä¸­åº”ç”±æ¨¡å‹ç”Ÿæˆ
        f_s = torch.randn((1, 64, h, w))  # å­¦ç”Ÿæ¨¡å‹ç‰¹å¾
        f_t = torch.randn((1, 64, h, w))  # æ•™å¸ˆæ¨¡å‹ç‰¹å¾
        return f_s, f_t
    
    f_s, f_t = get_dummy_features(h=28, w=28)
    att_s = compute_attention_map(f_s, p=2)[0].detach().numpy()  # Ensure it's a single map and 2D
    att_t = compute_attention_map(f_t, p=2)[0].detach().numpy()  # Ensure it's a single map and 2D

    fig, axs = plt.subplots(1, 2, figsize=(8, 3.5))
    im0 = axs[0].imshow(att_t, cmap='jet', interpolation='bilinear')
    axs[0].set_title('Teacher Attention Map', fontsize=12)
    axs[0].axis('off')
    plt.colorbar(im0, ax=axs[0], fraction=0.046, pad=0.04)

    im1 = axs[1].imshow(att_s, cmap='jet', interpolation='bilinear')
    axs[1].set_title('Student Attention Map (before AT)', fontsize=12)
    axs[1].axis('off')
    plt.colorbar(im1, ax=axs[1], fraction=0.046, pad=0.04)

    plt.tight_layout()
    plt.savefig('at_attention_comparison.png', dpi=150, bbox_inches='tight')
    print("âœ… Saved: at_attention_comparison.png")
# ----------------------------
# å›¾2ï¼šAT è’¸é¦æµç¨‹å›¾
# ----------------------------
def plot_at_pipeline():
    fig, ax = plt.subplots(figsize=(10, 4))
    ax.axis('off')

    # æ•™å¸ˆåˆ†æ”¯
    ax.text(0.2, 0.7, 'Teacher Network', ha='center', fontsize=12, weight='bold')
    ax.add_patch(plt.Rectangle((0.05, 0.55), 0.3, 0.1, fill=None, edgecolor='blue'))
    ax.text(0.2, 0.6, 'Feature Map $F_t$', ha='center', fontsize=11)

    # å­¦ç”Ÿåˆ†æ”¯
    ax.text(0.2, 0.3, 'Student Network', ha='center', fontsize=12, weight='bold')
    ax.add_patch(plt.Rectangle((0.05, 0.15), 0.3, 0.1, fill=None, edgecolor='orange'))
    ax.text(0.2, 0.2, 'Feature Map $F_s$', ha='center', fontsize=11)

    # æ³¨æ„åŠ›è®¡ç®—
    ax.annotate('', xy=(0.45, 0.6), xytext=(0.35, 0.6), arrowprops=dict(arrowstyle='->', color='blue'))
    ax.text(0.5, 0.6, 'Attention\n$A_t = \\|F_t\\|_p^p$', ha='center', fontsize=11, color='blue')

    ax.annotate('', xy=(0.45, 0.2), xytext=(0.35, 0.2), arrowprops=dict(arrowstyle='->', color='orange'))
    ax.text(0.5, 0.2, 'Attention\n$A_s = \\|F_s\\|_p^p$', ha='center', fontsize=11, color='orange')

    # æŸå¤±å‡½æ•°
    ax.annotate('', xy=(0.7, 0.4), xytext=(0.6, 0.6), arrowprops=dict(arrowstyle='->', color='blue'))
    ax.annotate('', xy=(0.7, 0.4), xytext=(0.6, 0.2), arrowprops=dict(arrowstyle='->', color='orange'))
    ax.text(0.75, 0.4, '$\\mathcal{L}_{AT} = \\|A_t - A_s\\|_2^2$', ha='center', fontsize=12, weight='bold')

    plt.title('Attention Transfer (AT) Knowledge Distillation', fontsize=14, y=0.95)
    plt.savefig('at_pipeline.png', dpi=150, bbox_inches='tight')
    print("âœ… Saved: at_pipeline.png")

# ----------------------------
# å›¾3ï¼šç‰¹å¾å›¾ â†’ æ³¨æ„åŠ›å›¾è½¬æ¢
# ----------------------------
def plot_feature_to_attention():
    # æ¨¡æ‹Ÿä¸€ä¸ª 3x3 ç‰¹å¾å›¾ï¼ˆ3é€šé“ï¼‰
    np.random.seed(0)
    feat = np.random.rand(3, 32, 32)
    feat = (feat - feat.min()) / (feat.max() - feat.min())  # normalize to [0,1]

    # è®¡ç®—æ³¨æ„åŠ›ï¼šsum of squares across channels
    attention = np.sum(feat ** 2, axis=0)

    fig, axs = plt.subplots(2, 4, figsize=(10, 5))
    for i in range(3):
        axs[0, i].imshow(feat[i], cmap='viridis')
        axs[0, i].set_title(f'Channel {i+1}', fontsize=10)
        axs[0, i].axis('off')
    
    axs[0, 3].axis('off')  # ç©ºç™½

    # ç¬¬äºŒè¡Œï¼šæ³¨æ„åŠ›å›¾
    for i in range(3):
        axs[1, i].axis('off')
    im = axs[1, 1].imshow(attention, cmap='hot')
    axs[1, 1].set_title('Sum of Squares\n(Across Channels)', fontsize=10)
    plt.colorbar(im, ax=axs[1, 1], fraction=0.046, pad=0.04)

    axs[1, 0].text(0.5, 0.5, 'â†’', fontsize=20, ha='center', va='center')
    axs[1, 0].axis('off')
    axs[1, 2].axis('off')
    axs[1, 3].axis('off')

    plt.suptitle('From Multi-channel Features to Spatial Attention Map', fontsize=13)
    plt.tight_layout()
    plt.savefig('at_feature_to_attention.png', dpi=150, bbox_inches='tight')
    print("âœ… Saved: at_feature_to_attention.png")

# ----------------------------
# ä¸»ç¨‹åº
# ----------------------------
if __name__ == "__main__":
    plot_attention_comparison()
    plot_at_pipeline()
    plot_feature_to_attention()
    print("\nğŸ‰ All figures generated successfully!")